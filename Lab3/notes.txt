- GaussNB{

Pros & Cons (Naive Bayes):
1. They are extremely fast for both training and prediction.
2. They provide straightforward probabilistic prediction.
3. They are often easily interpretable.
4. They have few (if any) tunable parameters.
5. Good choice as an initial baseline classification.

Usage (NB):
1. When the naive assumptions actually matches the data (rare in practice)
2. For well-separated categories or high-dimensional data 
and model complexity is less important.
3. Usually very good when there is little or low similarity in the data.

What it does:
GNBC classifies data by looking at previous data,
calculating a possibility that this new data point fits into a known class,
using a predetermined assumption.
(Assumption based classification)
    6 steps of GaussNB:
1. Assumption of Normal Distribution
2. Training Phase
3. Probability Calculation
4. Class Prior Probability
5. Posterior Probability
6. Classification

}

- Face Completion with a multi-output Estimators{

1. The program compares different generative models for
the recreation of faces.
2. Of all the provided methods Extra trees seems to provide the most
accurate results with good image-quality and low noise.
3. Look at picture BIG DIFF
4. Implementation of Haar-like feature detection
could improve the results of random forest by making
the recreation more accurate thanks to the improved feature detection.
Possibly increasing the impact of maximum features.

}

-Validation Metrics
1. All scores seemingly improved
Non-shuffled:
Linear Regression
mean R2: 0.20 (+/- 1.19)
MSE: 34.54

Ridge Regression
mean R2: 0.26 (+/- 1.04)
MSE: 33.91

Lasso Regression
mean R2: 0.26 (+/- 0.99)
MSE: 34.01

Decision Tree regression
mean R2: -0.03 (+/- 1.73)
MSE: 33.82

Random Forest regression
mean R2: 0.46 (+/- 0.92)
MSE: 21.75

Linear Support Vector Machine regression
mean R2: 0.31 (+/- 1.10)
MSE: 32.39

Support Vector Machine regression rbf
mean R2: -0.40 (+/- 0.77)
MSE: 71.88

K-Nearest Neighbors regression
mean R2: -4.95 (+/- 25.12)
MSE: 107.66

Shuffled:
Linear Regression
mean R2: 0.70 (+/- 0.23)
MSE: 23.81

Ridge Regression
mean R2: 0.70 (+/- 0.24)
MSE: 23.99

Lasso Regression
mean R2: 0.69 (+/- 0.25)
MSE: 25.06

Decision Tree regression
mean R2: 0.74 (+/- 0.29)
MSE: 21.58

Random Forest regression
mean R2: 0.82 (+/- 0.21)
MSE: 14.76

Linear Support Vector Machine regression
mean R2: 0.68 (+/- 0.29)
MSE: 25.84

Support Vector Machine regression rbf
mean R2: 0.21 (+/- 0.27)
MSE: 67.33

K-Nearest Neighbors regression
mean R2: 0.51 (+/- 0.23)
MSE: 40.22

2. RFE with/out Shuffle
Unshuffled:
Linear Regression with RFE
mean R2: 0.25 (+/- 1.07)
MSE: 31.48

Ridge Regression with RFE
mean R2: 0.26 (+/- 1.08)
MSE: 31.04

Lasso Regression with RFE
mean R2: 0.18 (+/- 1.33)
MSE: 31.74

Decision Tree regression with RFE
mean R2: -0.04 (+/- 1.87)
MSE: 33.13

Random Forest regression with RFE
mean R2: 0.41 (+/- 1.00)
MSE: 21.67

Linear Support Vector Machine regression with RFE
mean R2: 0.23 (+/- 1.30)
MSE: 31.73

Linear Regression with RFE
mean R2: 0.69 (+/- 0.23)
MSE: 25.27

Ridge Regression with RFE
mean R2: 0.69 (+/- 0.24)
MSE: 25.39

Lasso Regression with RFE
mean R2: 0.67 (+/- 0.25)
MSE: 26.50

Decision Tree regression with RFE
mean R2: 0.67 (+/- 0.36)
MSE: 26.96

Random Forest regression with RFE
mean R2: 0.81 (+/- 0.23)
MSE: 15.37

Linear Support Vector Machine regression with RFE
mean R2: 0.68 (+/- 0.25)
MSE: 26.53

3. KNN performed the best overall but isn't optimal.
By adding and comparing a decision tree algorithm to the other methods, it's clear that the tree outperforms the others by a large margin.
This is due to its ability to handle non-linearity and more complex relationships between variables.