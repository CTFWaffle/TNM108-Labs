Questions for Part1:
1. When can you use linear regression?
2. How can you generalize linear regression models to account for more complex relationships
among the data?
3. What are the basis functions?
4. How many basis functions can you use in the same regression model?
5. Can overfitting be a problem? And if so, what can you do about it?

Answers for Part1:
1. When you want to see linear relations between datasets.
2. By adding in a pipeline, more complex relationships among the data can more easily be 
accounted for by allowing higher dimensionality.
3. They're the mathematical models that can best be fitted to our data or describe it.
4. Several basis functions can be used to describe a more complex function, resulting in one either
very complex function or somewhat simple. There is no theoretical limit but rather a technical one.
5. By implementing some form of regularization you can minimize overfitting, there are many 
regularization methods: Ridge and Lasso are mentioned in the lab for example. Simplifying the 
model by making them more generalized, making some datapoints more and less important.

Questions for Part2:
1. Why choosing a good value for k is important in KNN?
2. How can you decide a good value for k?
3. Can you use KNN to classify non-linearly separable data?
4. Is KNN sensible to the number of features in the dataset?
5. Can you use KNN for a regression problem?
6. What are the Pros and Cons of KNN?

Answers for Part2:
1. A high value of K will result in overfitting, while a low value of K will only
represent the noise. You want a relatively smooth decision-boundary.
2. Avoid using a K that is equal to a multiple of the number of classes. Or use the Elbow method, using 
the breakingpoint between error and false-positive.
3. YES! The datasets don't need to be linearly related.
4. For an increasing amount of features KNN will require more data, different features may also
require different weights to scale the different data points for equal treatment.
5. You need different classes for KNN to be useful, so for regression problems it won't necessarily be
applicable.
6. 
Pros: 
-Easily applicable
-Much faster in the general case
-Low amount of Hyperparameters
-Easily evaluated.
Cons: 
-Requires the whole dataset
-Needs large number of samples for accuracy 
-More features requires more data
-Due to low amount of hyperparameters, K is HYPER sensitive.

Questions for Part3:
1. What is the basic idea/intuition of SVM?
2. What can you do if the dataset is not linearly separable?
3. Explain the concept of Soften Margins
4. What are the pros and cons of SVM?

Answers for Part3
1. Drawing a "line" to separate different features.
2. You can project to a higher dimension where the linear seperability is more apparent.
3. You allow some points to pass over the support vectors, within the margin.
4. 
Pros: 
- Works really well with clear margins.
- Effective with high dimensionanlity.
- Many use cases for different types of data.
- Works for non-linear boundaries.
Cons:
- Performs badly with large data.
- Doesn't perform well with large amounts of noise.
- No built-in probability estimates.
- C is a manually tweaked hyperparameter